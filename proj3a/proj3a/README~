NAME: Stewart Dulaney
EMAIL: sdulaney@ucla.edu
ID: 904-064-791

Included files:

SortedList.h:
- a header file containing interfaces for linked list operations

SortedList.c:
- implements insert, delete, lookup, and length methods for a sorted doubly linked list

lab2_list.c:
- implements the specified command line options (--threads, --iterations, --yield, --sync, --lists), drives one or more parallel threads that do operations on a shared linked list, and reports on the final list and performance
- usage: ./lab2_list [OPTION]...
- valid options: --threads=# (default 1), --iterations=# (default 1), --yield=[idl], --sync=[ms], --lists=# (default 1)

Makefile:
- supports the following targets:
  build (default) ... build the lab2_list executable
  lab2_list ... build the lab2_list executable
  tests ... run all specified test cases to generate CSV results
  profile ... run tests with profiling tools to generate an execution profiling report
  graphs ... use gnuplot to generate the required graphs
  dist ... create the deliverable tarball
  clean ... delete all programs and output generated by the Makefile

lab2b_list.csv:
- results for all of test runs

profile.out:
- execution profiling report showing where time was spent in the un-partitioned spin-lock implementation

lab2b_1.png:
- throughput vs. number of threads for mutex and spin-lock synchronized list operations

lab2b_2.png:
- mean time per mutex wait and mean time per operation for mutex-synchronized list operations

lab2b_3.png:
- successful iterations vs. threads for each synchronization method

lab2b_4.png:
- throughput vs. number of threads for mutex synchronized partitioned lists

lab2b_5.png:
- throughput vs. number of threads for spin-lock-synchronized partitioned lists

lab2_list.gp:
- script to read lab2b_list.csv and produce the .png graphs listed above

test_list.sh:
- shell script to generate results of test runs

Testing Methodology:
- The shell script test_list.sh is run by the tests target of the Makefile and it's output is redirected to lab2b_list.csv.

QUESTION 2.3.1 - Cycles in the basic list implementation:
- Where do you believe most of the cycles are spent in the 1 and 2-thread list tests?
In the 1 and 2-thread list tests, it seems reasonable to assume that most of the cycles are spent actually performing the list operations in the critical section.
- Why do you believe these to be the most expensive parts of the code?
If there are only 1 or 2 threads, there is less lock contention and so it seems reasonable to assume that less execution time (compared with high-thread scenarios) is being spent getting the locks. On the other hand, list operations on a sorted linked list are O(n) in the size of the list and that processing time would be the most expensive if synchronization overhead is low.
- Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
In high-thread spin-lock tests, most of the cycles would be spent spinning and checking if the lock is available if lock contention is high enough.
- Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
In high-thread mutex tests, most of the time/cycles are spent on either context switches or list operations. While context switches are very expensive, more cycles could be spent on list operations if the list was long enough (linear time operations). On the other hand, if the list is short, more cycles would be spent on switching from user mode to kernel mode and back for a context switch.

QUESTION 2.3.2 - Execution Profiling:
- Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
When the spin-lock version of the list exerciser is run with a large number of threads, most of the cycles are consumed by the lines of code checking if the spin lock is available (i.e., the while loop that spins and runs __sync_lock_test_and_set until the lock is available):
while (__sync_lock_test_and_set(&spin_lock, 1)) {
      continue;
}
This accounts for 408/458 * 100 = 89.1% of samples, while the remaining (only) 10.9% are running SortedList_lookup.
- Why does this operation become so expensive with large numbers of threads?
With large numbers of threads this operation becomes very expensive because with high lock contention all other threads spend cycles spinning and checking the spin lock until a single thread finishes all of its work on the list (the critical section). So threads spend a lot of time spinning in the loop above, waiting to get the lock for the critical section.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
- Why does the average lock-wait time rise so dramatically with the number of contending threads?
As the number of contending threads increases, it is more likely that the lock will be unavailable when a thread needs it. In addition, a pool/queue of multiple threads may be waiting on the same lock, causing the dramatic increase in time to get a lock. For example, a thread could have to wait while all other threads get the lock and execute their critical section first, depending on the algorithm for contending threads.
- Why does the completion time per operation rise (less dramatically) with the number of contending threads?
As lock-wait time increases with the number of contending threads, time per operation must also rise because the time to execute the critical section stays the same. The increased overhead of mutex synchronization and the increase in lock-wait time are responsible for the increased completion time per operation.
- How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
The completion time per operation is calculated in the parent thread using wall time. On the other hand, lock-wait time per operation is calculated in each individual thread created by the parent thread. The time periods of waiting of individual threads can overlap when multiple threads are waiting at the same time, which explains how the lock-wait time per operation can go up faster/higher than the completion time per operation. When a nanosecond is added to the total completion time, multiple nanoseconds can be added to the total mutex wait time when multiple threads are waiting.

QUESTION 2.3.4 - Performance of Partitioned Lists
- Explain the change in performance of the synchronized methods as a function of the number of lists.
As the number of lists increases the throughput or total number of operations per second also increases. This is because increasing the number of lists decreases the contention for the lock of each list, list operations become faster because the length of each list becomes shorter, and parallelism increases since more threads can operate on the list at once.
- Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
The throughput should continue increasing with the number of lists up to a limit. This limit could be caused by contention approaching zero so adding lists doesn't reduce contention, each list element being inserted into its own list so adding lists doesn't speed up list operations, or the finite amount of CPU cores preventing adding lists from increasing parallelism.
- It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.
Note: I interpreted "...single list with fewer (1/N) threads..." as "...single list with fewer (1 vs. N) threads..."
No, because of the factor that partitioning the list speeds up list operations because the length of each list becomes shorter, thus shortening time spent in the critical section. So even though a single list with a single thread doesn't have contention and is constantly operating on the list, it has lower throughput than an 8-way partitioned list with 8 threads because operations on the 8-way partitioned multilist are faster. This can be seen in both the mutex and spin lock curves, where throughput for 1-list/1-thread is < 1x10^6 operations per second, and throughput for 8-list/8-threads is > 1x10^6 operations per second.

